# Model Provider Configuration

# Built-in providers
providers:
  ollama:
    base_url: "http://127.0.0.1:11434"
    enabled: true
    platform: ["windows", "linux", "macos"]
    exclusive_instance: true
    launch_command:
      windows: "start /b ollama serve"
      linux: "ollama serve &"
      macos: "ollama serve &"
    models:
      - id: "llama3"
        name: "Llama 3"
        capabilities: ["text_generation", "reasoning"]
        resource_requirements:
          ram: "8GB"
          vram: "0GB"
      - id: "codellama"
        name: "Code Llama"
        capabilities: ["code_generation", "code_completion"]
        resource_requirements:
          ram: "8GB"
          vram: "0GB"
      - id: "mistral"
        name: "Mistral"
        capabilities: ["text_generation", "reasoning"]
        resource_requirements:
          ram: "6GB"
          vram: "0GB"

  vllm:
    base_url: "http://localhost:8000"
    enabled: true
    platform: ["linux", "windows_wsl"]
    exclusive_instance: true
    shared_instance: true  # Single instance shared by all agents
    launch_command:
      linux: "./start_vllm_optimized.sh small"
      windows_wsl: "start_vllm_quick.bat"
    models:
      - id: "distilgpt2"
        name: "DistilGPT2 (Fast)"
        capabilities: ["text_generation", "reasoning"]
        resource_requirements:
          ram: "2GB"
          vram: "0GB"
        cpu_optimized: true
      - id: "gpt2"
        name: "GPT2 (Balanced)"
        capabilities: ["text_generation", "reasoning", "code_completion"]
        resource_requirements:
          ram: "3GB"
          vram: "0GB"
        cpu_optimized: true
      - id: "gpt2-medium"
        name: "GPT2-Medium (Quality)"
        capabilities: ["text_generation", "reasoning", "code_completion"]
        resource_requirements:
          ram: "4GB"
          vram: "0GB"
        cpu_optimized: true
      - id: "microsoft/CodeGPT-small-py"
        name: "CodeGPT-Small-Py (Code)"
        capabilities: ["code_generation", "code_completion", "text_generation"]
        resource_requirements:
          ram: "3GB"
          vram: "0GB"
        cpu_optimized: true
        capabilities: ["text_generation", "code_completion"]
        resource_requirements:
          ram: "6GB"
          vram: "6GB"

  lmstudio:
    base_url: "http://localhost:1234"
    enabled: true
    platform: ["windows", "macos"]
    exclusive_instance: true
    launch_command:
      windows: "start /b \"LM Studio\" \"C:\\Program Files\\LM Studio\\LM Studio.exe\""
      macos: "open -a \"LM Studio\""
    models:
      - id: "local-model"
        name: "LM Studio Local Model"
        capabilities: ["text_generation", "reasoning", "code_generation"]
        resource_requirements:
          ram: "8GB"
          vram: "0GB"
      - id: "code-model"
        name: "LM Studio Code Model"
        capabilities: ["code_generation", "code_completion"]
        resource_requirements:
          ram: "8GB"
          vram: "0GB"
      - id: "chat-model"
# Model Provider Configuration - Optimized for 8GB VRAM Systems

# Built-in providers
providers:
  ollama:
    base_url: "http://127.0.0.1:11434"
    enabled: true
    platform: ["windows", "linux", "macos"]
    exclusive_instance: true
    launch_command:
      windows: "start /b ollama serve"
      linux: "ollama serve &"
      macos: "ollama serve &"
    models:
      # Optimized for 8GB VRAM - Latest models
      - id: "llama3.2:3b"
        name: "Llama 3.2 3B"
        capabilities: ["text_generation", "reasoning", "code_assistance"]
        resource_requirements:
          vram: "2.0GB"
          priority: "high"
      - id: "phi3:mini"
        name: "Phi-3 Mini"
        capabilities: ["text_generation", "reasoning", "code_generation"]
        resource_requirements:
          vram: "2.3GB"
          priority: "high"
      - id: "gemma2:2b"
        name: "Gemma 2 2B"
        capabilities: ["text_generation", "reasoning"]
        resource_requirements:
          vram: "1.6GB"
          priority: "medium"
      - id: "codellama:7b"
        name: "Code Llama 7B"
        capabilities: ["code_generation", "code_completion", "code_review"]
        resource_requirements:
          vram: "3.8GB"
          priority: "high"
      - id: "deepseek-coder:6.7b"
        name: "DeepSeek Coder 6.7B"
        capabilities: ["code_generation", "code_completion", "debugging"]
        resource_requirements:
          vram: "3.9GB"
          priority: "high"
      - id: "starcoder2:3b"
        name: "StarCoder2 3B"
        capabilities: ["code_generation", "code_completion"]
        resource_requirements:
          vram: "1.7GB"
          priority: "medium"
      - id: "mistral:7b"
        name: "Mistral 7B"
        capabilities: ["text_generation", "reasoning", "analysis"]
        resource_requirements:
          vram: "4.1GB"
          priority: "medium"
      - id: "qwen2.5:7b"
        name: "Qwen 2.5 7B"
        capabilities: ["text_generation", "reasoning", "multilingual"]
        resource_requirements:
          vram: "4.3GB"
          priority: "medium"

  lmstudio:
    base_url: "http://localhost:1234"
    enabled: true
    platform: ["windows", "macos", "linux"]
    exclusive_instance: true
    launch_command:
      windows: "start /b \"LM Studio\" \"C:\\Program Files\\LM Studio\\LM Studio.exe\""
      macos: "open -a \"LM Studio\""
      linux: "lmstudio &"
    models:
      # 8GB VRAM optimized models
      - id: "microsoft-phi-3-mini"
        name: "Microsoft Phi-3 Mini"
        capabilities: ["text_generation", "reasoning", "code_generation"]
        resource_requirements:
          vram: "2.3GB"
          priority: "high"
      - id: "meta-llama-3.1-8b"
        name: "Meta Llama 3.1 8B"
        capabilities: ["text_generation", "reasoning", "code_assistance"]
        resource_requirements:
          vram: "4.7GB"
          priority: "medium"
      - id: "mistral-7b-instruct"
        name: "Mistral 7B Instruct"
        capabilities: ["text_generation", "reasoning", "instruction_following"]
        resource_requirements:
          vram: "4.1GB"
          priority: "high"
      - id: "codellama-7b-instruct"
        name: "CodeLlama 7B Instruct"
        capabilities: ["code_generation", "code_completion", "debugging"]
        resource_requirements:
          vram: "3.8GB"
          priority: "high"
      - id: "gemma-2-2b-it"
        name: "Gemma 2 2B Instruct"
        capabilities: ["text_generation", "reasoning"]
        resource_requirements:
          vram: "1.6GB"
          priority: "medium"
      - id: "qwen2.5-7b-instruct"
        name: "Qwen 2.5 7B Instruct"
        capabilities: ["text_generation", "reasoning", "multilingual"]
        resource_requirements:
          vram: "4.3GB"
          priority: "medium"
      - id: "deepseek-coder-6.7b"
        name: "DeepSeek Coder 6.7B"
        capabilities: ["code_generation", "code_completion", "code_review"]
        resource_requirements:
          vram: "3.9GB"
          priority: "high"

  vllm:
    base_url: "http://localhost:8000"
    enabled: true
    platform: ["linux", "windows_wsl"]
    exclusive_instance: true
    shared_instance: true
    launch_command:
      linux: "./start_vllm_optimized.sh small"
      windows_wsl: "start_vllm_quick.bat"
    models:
      # High-performance inference models
      - id: "microsoft/phi-3-mini"
        name: "Phi-3 Mini (vLLM)"
        capabilities: ["text_generation", "reasoning", "code_generation"]
        resource_requirements:
          vram: "2.3GB"
          priority: "high"
      - id: "meta-llama/Llama-3.1-8B"
        name: "Llama 3.1 8B (vLLM)"
        capabilities: ["text_generation", "reasoning", "code_assistance"]
        resource_requirements:
          vram: "4.7GB"
          priority: "medium"
      - id: "mistralai/Mistral-7B-Instruct"
        name: "Mistral 7B Instruct (vLLM)"
        capabilities: ["text_generation", "reasoning", "instruction_following"]
        resource_requirements:
          vram: "4.1GB"
          priority: "high"
      - id: "Qwen/Qwen2.5-7B-Instruct"
        name: "Qwen 2.5 7B Instruct (vLLM)"
        capabilities: ["text_generation", "reasoning", "multilingual"]
        resource_requirements:
          vram: "4.3GB"
          priority: "medium"
        resource_requirements:
          ram: "6GB"
          vram: "0GB"

# Plugin providers
plugins:
  example_provider:
    enabled: true
    config:
      base_url: "http://localhost:8000"
      api_key: ""
      timeout: 30

# Default provider and model
default_provider: "ollama"
default_model: "llama3"

# Agent model assignments (CPU-optimized for multiple local endpoints)
agent_assignments:
  architect:
    primary: ["lmstudio/mistral-small-3.1-24b-instruct-2503", "ollama/mistral"]
    fallback: ["lmstudio/gemma-2b-it", "ollama/llama3"]
    task_specific:
      system_design: ["lmstudio/mistral-small-3.1-24b-instruct-2503"]
      architecture_review: ["ollama/mistral"]
      
  backend_dev:
    primary: ["lmstudio/codellama-7b-instruct", "ollama/codellama"]
    fallback: ["lmstudio/mistral-small-3.1-24b-instruct-2503", "ollama/llama3"]
    task_specific:
      api_development: ["lmstudio/codellama-7b-instruct"]
      database_design: ["ollama/llama3"]
      
  frontend_dev:
    primary: ["lmstudio/codellama-7b-instruct", "ollama/codellama"]
    fallback: ["lmstudio/mistral-small-3.1-24b-instruct-2503", "ollama/llama3"]
    task_specific:
      ui_design: ["lmstudio/mistral-small-3.1-24b-instruct-2503"]
      
  qa_analyst:
    primary: ["lmstudio/mistral-small-3.1-24b-instruct-2503", "ollama/llama3"]
    fallback: ["lmstudio/gemma-2b-it", "ollama/mistral"]
    task_specific:
      test_planning: ["lmstudio/mistral-small-3.1-24b-instruct-2503"]
      
  orchestrator:
    primary: ["lmstudio/mistral-small-3.1-24b-instruct-2503", "ollama/mistral"]
    fallback: ["lmstudio/gemma-2b-it", "ollama/llama3"]
    
  workspace:
    primary: ["lmstudio/codellama-7b-instruct", "ollama/codellama"]
    fallback: ["ollama/llama3"]

# Agent-to-Model Assignments - Optimized for 8GB VRAM
agents:
  orchestrator:
    primary_models:
      - "ollama/phi3:mini"           # 2.3GB - Fast reasoning
      - "lmstudio/microsoft-phi-3-mini"  # Alternative
    fallback_models:
      - "ollama/gemma2:2b"          # 1.6GB - Ultra-fast fallback
    task_specific:
      coordination: ["ollama/phi3:mini"]
      planning: ["lmstudio/mistral-7b-instruct"]
      
  architect:
    primary_models:
      - "ollama/llama3.2:3b"        # 2.0GB - Good reasoning
      - "lmstudio/mistral-7b-instruct"  # 4.1GB - High quality
    fallback_models:
      - "ollama/phi3:mini"          # 2.3GB - Fast fallback
    task_specific:
      system_design: ["lmstudio/mistral-7b-instruct"]
      documentation: ["ollama/llama3.2:3b"]
      
  backend_dev:
    primary_models:
      - "ollama/codellama:7b"       # 3.8GB - Best for backend
      - "ollama/deepseek-coder:6.7b"  # 3.9GB - Strong coding
    fallback_models:
      - "ollama/starcoder2:3b"      # 1.7GB - Fast coding
    task_specific:
      api_development: ["ollama/codellama:7b"]
      database_design: ["ollama/deepseek-coder:6.7b"]
      debugging: ["lmstudio/deepseek-coder-6.7b"]
      
  frontend_dev:
    primary_models:
      - "ollama/codellama:7b"       # 3.8GB - UI/UX capable
      - "lmstudio/codellama-7b-instruct"  # 3.8GB - Alternative
    fallback_models:
      - "ollama/starcoder2:3b"      # 1.7GB - Fast prototyping
    task_specific:
      ui_design: ["ollama/llama3.2:3b"]
      component_development: ["ollama/codellama:7b"]
      styling: ["ollama/starcoder2:3b"]
      
  qa_agent:
    primary_models:
      - "ollama/deepseek-coder:6.7b"  # 3.9GB - Good for testing
      - "ollama/phi3:mini"          # 2.3GB - Fast analysis
    fallback_models:
      - "ollama/gemma2:2b"          # 1.6GB - Quick checks
    task_specific:
      code_review: ["ollama/deepseek-coder:6.7b"]
      test_generation: ["lmstudio/codellama-7b-instruct"]
      bug_detection: ["ollama/phi3:mini"]

# Performance Optimization Settings
performance:
  vram_optimization:
    enabled: true
    max_vram_gb: 7.5
    aggressive_cleanup: true
    model_rotation: true
    
  load_balancing:
    enabled: true
    max_concurrent_models: 2      # Conservative for 8GB
    priority_based_loading: true
    
  fallback_strategy:
    cloud_providers: ["openrouter"]  # Optional cloud fallback
    timeout_seconds: 10
    max_retries: 3
    
  caching:
    model_context_cache: true
    response_cache: false         # Disabled for development
    cache_size_mb: 500
    
# Model Selection Strategy
selection_strategy:
  method: "vram_aware"            # vram_aware, performance, quality
  prefer_local: true
  allow_cloud_fallback: false     # Set to true if you have API keys
  
  quality_thresholds:
    min_model_size_gb: 1.5        # Minimum viable model size
    prefer_instruct_models: true
    prefer_recent_models: true
    
  vram_constraints:
    single_large_model_only: true # Only one >4GB model at a time
    emergency_cleanup_threshold: 7.0  # GB
    force_unload_threshold: 7.5   # GB
    
# Cloud Provider Fallbacks (Optional)
cloud_providers:
  openrouter:
    api_key: "${OPENROUTER_API_KEY}"
    enabled: false
    models:
      - "microsoft/phi-3-mini-128k-instruct"
      - "meta-llama/llama-3.1-8b-instruct"
    rate_limits:
      requests_per_minute: 20
      
  huggingface:
    api_key: "${HUGGINGFACE_API_KEY}"
    enabled: false
    models:
      - "microsoft/DialoGPT-medium"
      - "bigcode/starcoder2-3b"
    rate_limits:
      requests_per_minute: 15